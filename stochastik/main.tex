\documentclass[a4paper,titlepage]{article}

% +--------------------------------+
% | Encoding & Language            |
% +--------------------------------+

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}

% +--------------------------------+
% | Layout                         |
% +--------------------------------+

\usepackage[margin = 2cm, includeheadfoot]{geometry}

% +--------------------------------+
% | Header                         |
% +--------------------------------+

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Wahrscheinlichkeit und Statistik}
\fancyhead[C]{Nicolas Trüssel}
\fancyhead[R]{Seite \thepage}

% +--------------------------------+
% | Math                           |
% +--------------------------------+

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

% +--------------------------------+
% | Operators                      |
% +--------------------------------+

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}

% +--------------------------------+
% | Custom Commands                |
% +--------------------------------+

\newcommand{\set}[1]{\left\lbrace #1 \right\rbrace}
\newcommand{\emath}{\boldsymbol{e}}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}

% +--------------------------------+
% | Titlepage                      |
% +--------------------------------+

\title{Zusammenfassung Wahrscheinlichkeit und Statistik}
\author{Nicolas Trüssel}

\begin{document}
\maketitle

\section{Diskrete Verteilungen}
\subsection{Bernoulli Verteilung}
Sei $X \sim Be(p)$ (Erfolg 1, Misserfolg 0). Dann ist:
\begin{equation*}
P[X=x] = p^x(1-p)^x, x \in \set{0,1}, \qquad E(X) = p, \qquad \Var(X) = p(1-p), \qquad m_k = p
\end{equation*}

\subsection{Binomialverteilung}
Sei $X \sim Bin(n,p)$ (Anzahl Erfolge). Dann ist:
\begin{equation*}
P[X=k] = \binom{n}{k}p^k(1-p)^{n-k}, k \in \set{0,\dots,n}, \qquad E(X) = np, \qquad \Var(X) = np(1-p)
\end{equation*}

\subsection{Geometrische Verteilung}
Sei $X \sim Geom(p)$ (Wartezeit auf ersten Erfolg). Dann ist:
\begin{equation*}
P[X = k] = p(1-p)^{k-1}, k \in \set{1,2,3,\dots}, \qquad E(X) = \frac{1}{p}, \qquad \Var(X) = \frac{1-p}{p^2}
\end{equation*}

\subsection{Negativ-Binomialverteilung}
Sei $X \sim NB(r,p)$ (Wartezeit auf $r$-ten Erfolg). Dann ist:
\begin{equation*}
P[X = k] = \binom{k-1}{r-1}p^r(1-p)^{k-r}, k \in \set{r,r+1, \dots}, \qquad E(X) = \frac{r}{p}, \qquad \Var(X) = \frac{r(1-p)}{p^2}
\end{equation*}

\subsection{Hypergeometrische Verteilung}
Sei $X \sim HGeom(n,m,r)$. In einer Urne sind $r$ Kugeln mit einer Eigenschaft, $n-r$ Kugeln ohne diese Eigenschaft. Man zieht $m$ Kugeln ohne Zurücklegen. $X$ beschreibt die Anzahl der Kugeln mit der Eigenschaft. Dann ist:
\begin{equation*}
P[X = k] = \frac{\binom{r}{k} \binom{n-r}{m-k}}{\binom{n}{m}}, k \in \set{0,1 \dots \min(m,r)}, \qquad E(X) = m \frac{r}{n}, \qquad \Var(X) = m \frac{r}{n} \left( 1- \frac{r}{n}\right) \frac{n-m}{n-1}
\end{equation*}

\subsection{Poisson Verteilung}
Sei $X \sim Poi(\lambda)$ (für seltene Ereignisse benutzt). Dann gilt:
\begin{equation*}
P[X = k] = \emath^{-\lambda}\frac{\lambda^k}{k!}, k \in \set{0,1,2, \dots}, \qquad E(X) = \lambda, \qquad \Var(X) = \lambda
\end{equation*}
\subsubsection{Summe poisson-verteilter Zufalllsvariablen}
Sind $X_1 \sim Poi(\lambda_1), X_2 \sim Poi(\lambda_2)$ unabhängig, so ist $ X_1 + X_2 = X \sim Poi(\lambda_1 + \lambda_2)$.
\subsection{Beziehung zur Binomialverteilung}
Für grosse $n$ und kleine $p$ kann man die Binomial-Wahrscheinlichkeit approximativ durch die Poisson-Wahrschinlichkeit berechnen wobei $\lambda = np$.

\section{Stetige Verteilungen}
\subsection{Gleichverteilung}
Sei $X \sim \mathcal{U}(a,b)$. Dann ist 
\begin{equation*}
f_X(t) = \begin{cases} \frac{1}{b-a} & a \leq t \leq b \\ 0	& \text{sonst}
\end{cases}
\qquad
F_X(t) = \begin{cases}
0 &	t < a  \\
\frac{t-a}{b-a} & a \leq t \leq b \\
1 & t > b
\end{cases}
\qquad 
E(X) = \frac{a+b}{2}
\qquad
\Var(X) = \frac{1}{12}(b-a)^2
\end{equation*}

\subsection{Exponentialverteilung}
Sei $X \sim Exp(\lambda)$ (für Wartezeiten und Lebensdauern). Dann gilt:
\begin{equation*}
f_X(t) = \begin{cases}
\lambda \emath^{-\lambda t} & t \geq 0 \\
0 & t < 0
\end{cases} 
\qquad
F_X(t) = \begin{cases}
1-\emath^{-\lambda t} & t \geq 0 \\
0 & t < 0
\end{cases}
\qquad
E(X) = \frac{1}{\lambda}
\qquad
\Var(X) = \frac{1}{\lambda^2}
\end{equation*}

\subsection{Normalverteilung}
Sei $X \sim \mathcal{N}(\mu, \sigma^2)$. Dann ist:
\begin{equation*}
f_X(t) = \frac{1}{\sigma\sqrt{2 \pi}}\emath^{-\frac{(t-\mu)^2}{2\sigma^2}}, t \in \mathbb{R}, \qquad E(X) = \mu, \qquad \Var(X) = \sigma^2
\end{equation*}
\subsubsection{Standardnormalverteilung}
Ist $X \sim \mathcal{N}(\mu, \sigma^2)$, so ist $\dfrac{X-\mu}{\sigma} \sim \mathcal{N}(0,1)$. Also ist $F_X(t) = \Phi\left(\dfrac{t-\mu}{\sigma}\right)$
\subsubsection{Summe}
Sind $X \sim \mathcal{N}(\mu_X, \sigma_X^2), Y \sim \mathcal{N}(\mu_Y, \sigma_Y^2)$ unabhängig, so sind ist $X + Y \sim \mathcal{N}(\mu_X+ \mu_Y, \sigma_X^2 + \sigma_Y^2)$

\subsection{Pareto-Verteilung}
Sei $X \sim Par(\alpha,x_0) \quad \alpha,x_0 > 0$ (für Katastrophen). Dann ist:
\begin{equation*}
f_X(x) = \begin{cases}
\frac{\alpha}{x_0}\left( \frac{x_0}{x}\right)^{\alpha+1} & x \geq x_0 \\
0 & x < x_0
\end{cases}
\qquad
F_X(x) = 1 - \left( \frac{x_0}{x}\right)^\alpha, x \geq x_0 
\qquad E(X) = \begin{cases}
x_0\frac{\alpha}{\alpha - 1} & \alpha > 1 \\
\infty 	& \alpha < 1
\end{cases}
\end{equation*}

\section{Definitionen und Umformungen}
\subsection{Bedingte Wahrscheinlichkeit}
\subsubsection{Definition}
\begin{equation*}
P[A|B] := \frac{P[A \cap B]}{P[B]}
\end{equation*}
\subsubsection{Umformungen}
\begin{equation*}
P[A^C|B] = 1 - P[A|B]
\end{equation*}
Sind $A$ und $B$ unabhängig, so gilt:
\begin{equation*}
P[A \cap B |C] = P[A|C]P[B|C]
\end{equation*}

\subsection{Formel von Bayes}
Sei $A_1, \dots A_n$ eine Zerlegung des Grundraumes mit $P[A_i] > 0$ und $B$ ein Ereignis mit $P[B] > 0$. Dann gilt für jedes $k$:
\begin{equation*}
P[A_k|B] = \frac{P[B|A_k] P[A_k] } {\sum\limits_{i=1}^{n} P[B|A_i]P[A_i] = P[B]}
\end{equation*}

\subsection{Erwartungswert}
\subsubsection{Definition}
\begin{equation*}
E(X) = \sum_{x \in \mathcal{W}(X)} x P[X = x], \qquad E(X) = \int_{-\infty}^\infty x f_X(x) \d x
\end{equation*}
\subsubsection{Umformungen}
Sei $Y = g(X)$:
\begin{equation*}
E(Y) = \sum_{x \in \mathcal{W}(X)} g(x) P[X = x], \qquad E(Y) = \int_{-\infty}^\infty g(x) f_X(x) \d x
\end{equation*}
Insbesondere gilt:
\begin{equation*}
E(a X+b) = a E(X) + b, \qquad E(aX + bY) = aE(X) + bE(y)
\end{equation*}
\subsubsection{Summe von Zufallsvariablen}
\begin{equation*}
E\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n E \left( X_i \right)
\end{equation*}
\subsubsection{Produkt von Zufallsvariablen}
Sind $X_i \dots X_n$ unabhängig, so ist 
\begin{equation*}
E \left( \prod_{i=1}^n X_i \right) = \prod_{i=1}^n E\left(X_i\right)
\end{equation*}

\subsection{Varianz}
\subsubsection{Definition}
\begin{equation*}
\Var(X) = E\left( \left(X-E(X) \right)^2 \right)
\end{equation*}
\subsubsection{Umformungen}
Falls $E(X^2) < \infty$:
\begin{equation*}
\Var(X) = E(X^2) - E(X)^2, \qquad \Var(aX + b) = a^2 \Var(X)
\end{equation*}
\subsubsection{Summe von Zufallsvariablen}
\begin{equation*}
\Var\left( \sum_{i=1}^n X_i \right) = \sum_{i,j=1}^n \Cov(X_i,X_j) 
\end{equation*}

\subsection{Kovarianz}
\subsubsection{Definition}
\begin{equation*}
\Cov(X,Y) = E\left[ \left( X- E \left( X \right) \right) \cdot \left( Y - E\left( Y \right)\right) \right]
\end{equation*}
\subsubsection{Umformungen}
\begin{equation*}
\Cov(X,Y) = E(XY) - E(X)E(Y)
\end{equation*}
Die Kovarianz ist eine positiv semidefinite Bilinearform, d.h es gilt:
\begin{align*}
\Cov(aX +b, cY + d) &= ac\Cov(X,Y) \\
\Cov(X,(aY + b) + (cZ + d)) &= a\Cov(X,Y) + c \Cov(X,Z) \\
\Cov(X,Y) &= \Cov(Y,X) \\
\Cov(X,X) &\geq 0
\end{align*}

\subsection{Randverteilung}
Haben $X$ und $Y$ die gemeinsame Verteilungsfunktion $F$, so ist $F_X: \mathbb{R} \to [0,1]$,
\begin{equation*}
x \mapsto F_X(x) := \lim_{y \to \infty} F(x,y)
\end{equation*}
die Randverteilung von $X$. \\[0.5em]
Wenn $X$ und $X$ die gemeinsame Dichte $f$ ($p$ im diskreten Fall) haben, so haben auch die Randverteilungen Dichten:
\begin{equation*}
f_X(x) = \int_{-\infty}^{\infty}f(x,y) \d y, \qquad p_X(x) = \sum_{y \in \mathcal{W}(Y)} p(x,y)
\end{equation*}

\subsection{Zentraler Grenzwertsatz}
Sei $X_1, X_2, \dots$ eine Folge von i.i.d. Zufallsvariablen mit $E(X_i) = \mu, \Var(X_i) = \sigma ^2$, $S_n = \sum_{i=1}^n X_i$. Dann ist:
\begin{equation*}
E(S_n) = n\mu, \quad \Var(S_n) = n\sigma^2, \quad S_n^* = \frac{S_n - n \mu}{\sigma \sqrt{n}} \overset{\text{approx}}{\sim} \mathcal{N}(0,1), \quad \text{für $n$ gross}
\end{equation*}

\section{Ungleichungen}
\subsection{Markov-Ungleichung}
Sei $X$ eine Zufallsvariable und $g: \mathcal{W}(X) \to [0,\infty)$ eine wachsende Funktion. Für alle $c \in \mathbb{R}$ mit $g(c) > 0$ gilt:
\begin{equation*}
P[X \geq c] \leq \frac{E(g(X))}{g(c)}
\end{equation*}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   

\subsection{Chebyshev-Ungleichung}
Sei $X$ eine Zufallsvariable mit endlicher Varianz. Für jedes $b > 0$ gilt:
\begin{equation*}
P[\abs{X-E(X)} \geq b] \leq \frac{\Var(X)}{b^2}
\end{equation*}

\subsection{Chernoff-Schranken}
Seien $X_1, \dots, X_n \sim Be(p_i)$ unabhängig und $S_n = \sum\limits_{i=1}^{n}X_i, \, \mu_n = E[S_n] = \sum\limits_{i=1}^{n}p_i$ und $\delta > 0$. Dann gilt:
\begin{equation*}
P[S_n \geq (1 + \delta) \mu_n] \leq \left( \frac{\emath^\delta}{\left( 1 + \delta \right)^{1+ \delta}}\right)^{\mu_n}
\end{equation*}

\subsubsection{Allgemeinere Ungleichung}
Seien $X_1, \dots, X_n$ i.i.d. Zufallsvariablen, für welche die momenterzeugende Funktion $M_X(t)$ für alle $t \in \mathbb{R}$ endlich ist ($M_X(t) := E(\emath^{tX})$). Für jedes $b \in \mathbb{R}$ gilt:
\begin{equation*}
P[S_n \geq b] \leq \exp\left( \inf\limits_{t \in \mathbb{R}} \left( n \log M_X(t) -tb \right) \right)
\end{equation*}

\subsection{Schwaches Gesetz der Grossen Zahlen}
Sei $X_1, X_2, \dots$ eine Folge von unabhängigen Zufallsvariablen mit dem selben Erwartungswert $\mu$ und der gleichen Varianz $\sigma^2$. Dann gilt:
\begin{equation*}
P \left[ \abs{ \overline{X}_n - \mu } > \varepsilon \right] \xrightarrow[n \to \infty]{ } 0, \quad \text{für jedes $\varepsilon > 0$}
\end{equation*}
Das heisst, dass mit beliebig grosser Wahrscheinlichkeit der Wert von $\overline{X}_n$ für hinreichend grosse $n$ beliebig nahe bei $\mu$ liegt.

\subsection{Starkes Gesetz der Grossen Zahlen}
Sei $X_1, X_2, \dots$ eine Folge von unabhängigen Zufallsvariablen mit der selben Verteilung und endlichem Erwartungswert $\mu$. Es gilt: 
\begin{equation*}
\overline{X}_n \xrightarrow[n \to \infty]{} \mu,\quad \text{P-fastsicher}, \qquad\text{d.h.}\qquad P\left[ \set{\omega \in \Omega \vert \overline{X}_n(\omega) \xrightarrow[n \to \infty]{} \mu} \right] = 1
\end{equation*}
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
\section{Schätzer}

\subsection{Eigenschaften}
\subsubsection{Erwartungstreu}
Ein Schätzer $T$ heisst erwartungstreu für $\vartheta$, falls gilt $E_\vartheta[T]-\vartheta = 0$, die linke Seite dieser Gleichung heisst Bias.

\subsubsection{Mean Square Error}
Als Mean Square Error bezeichnet man
$$
MSE_\vartheta(T) := E_\vartheta\left((T-\vartheta)^2\right)
$$
\subsubsection{Konsistenz}
Eine Folge von Schätzern $T^{(n)}$ ist konsistent für $\vartheta$, falls 
$$
\forall \varepsilon > 0: \lim_{n \to \infty} P_\vartheta \left[ \abs{T^{(n)} - \vartheta} > \varepsilon\right] = 0  
$$
\subsection{Maximum Likelihood}
Die Likelihood-Funktion ist:
\begin{equation*}
L(x_1, \dots x_n ; \vartheta) := 
\begin{cases}
p(x_1, \dots x_n ; \vartheta) & \text{im diskreten Fall} \\
f(x_1, \dots x_n ; \vartheta) & \text{im stetigen Fall}
\end{cases}
\end{equation*}
Um den Paramteter $\vartheta$ zu schätzen, suche das Maximum der Likelihood-Funktion (meist zuerst den Logarithmus der ML-Funktion bestimmen und dann ableiten).

\subsubsection{Bernoulli-Verteilung}
Seien $X_1, \dots, X_n \overset{i.i.d.}{\sim} Be(p)$ der ML-Schätzer für $\vartheta = p$ ist 
\begin{equation*}
\hat{\vartheta} = \frac{1}{n}\sum_{i=1}^n X_i = \overline{X}_n
\end{equation*}

\subsubsection{Normalverteilung}
Seien $X_1, \dots, X_n \overset{i.i.d.}{\sim} \mathcal{N}(\mu,\sigma^2)$ ($\mu$ und $\sigma$ sind unbekannt). Dann ist der ML-Schätzer für $\vartheta=\begin{pmatrix} \mu & \sigma^2 \end{pmatrix}^T$:
\begin{equation*}
\hat{\vartheta} = 
\begin{pmatrix}
\overline{X}_n \\ \frac{1}{n} \sum\limits_{i=1}^n\left(X_i^2\right) -  {\overline{X}_n }^2 
\end{pmatrix}
\end{equation*}
Da der Schätzer für die Varianz nicht erwartunstreu ist wählt man für $\sigma^2$ meist den Schätzer
\begin{equation*}
S^2 := \frac{1}{n-1} \sum_{i=1}^n\left( X_i-\overline{X}_n \right)^2
\end{equation*}

\subsection{Momentenschätzer}
\subsubsection{Definition}
Sei $(X,)X_1,\dots, X_n \overset{i.i.d.}{\sim}F(\,\cdot\,;\vartheta), \vartheta \in \Theta \subset \mathbb{R}^m$
Das k-te Moment ist definiert als:
\begin{equation*}
m_k := E_\vartheta\left(X^k\right)=m_k(\vartheta)
\end{equation*}
Um die $m_k$ zu schätzen kann man die Momentenschätzer verwenden:
\begin{equation*}
\hat{m}_{k,n} = \frac{1}{n}\sum_{i=1}^kx_i^k
\end{equation*}
Um daraus $\vartheta$ zu erhalten setze $\hat{m}_{k,n} = m_k(\vartheta)$ und löse nach $\vartheta$ auf. Dazu müssen die Parameter als Funktion der Momente formuliert werden, zum Beispiel $\Var(X) = E(X^2) - E(X)^2 = m_2 - m_1^2$

\section{Tests}
\subsection{Bestimmung des kritischen Bereiches}
Für ein gegebenes Signifikanzniveau $\alpha$, eine Teststatistik $T$ und Nullhypothese $\Theta_0$ kann der kritische Bereich $K$ (bei bekannter Form) so bestimmt werden:
\begin{equation*}
\sup\limits_{\vartheta \in \Theta_0} P_\vartheta[T \in K] \leq \alpha
\end{equation*}

\subsection{Likelihood-Quotient}
Für feste $\vartheta_0$ und $\vartheta_A$ ist der Likelihood-Quotient ideal um eine Teststatistik zu finden:
\begin{equation*}
R(x_1,\dots,x_n ; \vartheta_0, \vartheta_A) := \frac{L(x_1,\dots,x_n; \vartheta_0}{L(x_1,\dots,x_n;\vartheta_A)}
\end{equation*}
Der kritische Bereich hat die Form $[0, c)$. Mit Hilfe des Supremums für Zähler und Nenner, kann diese Methode auch für kompliziertere Hypothesen ausgeweitet werden.

\subsection{Z-Test}
Seien $X_1, \dots, X_n \sim \mathcal{N}(\vartheta, \sigma^2)$ mit $\sigma$ bekannt und $H_0: \vartheta = \vartheta_0$. Dann ist eine geeignete Teststatistik
\begin{equation*}
Z := \frac{\overline{X}_n-\vartheta_0}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1), \quad \text{unter $P_{\vartheta_0}$}
\end{equation*}
Die Form des kritischen Bereiches $K$ hängt von der Alternativhypothese ab: Für $H_A: \vartheta > \vartheta_0$ ist $K = (c, \infty)$, für $H_A: \vartheta < \vartheta_0$ ist $K = (- \infty, c)$ und für $H_A: \vartheta \neq \vartheta_0$ ist $K = (- \infty, -c) \cup (c, \infty)$.

\subsection{T-Test}
Selbe Voraussetzungen wie Z-Test, jedoch ist $\sigma$ unbekannt. \\
Zuerst wird die Varianz mit dem Schätzer $S^2 := \frac{1}{n-1} \sum_{i=1}^n\left( X_i-\overline{X}_n \right)^2$ bestimmt. Die Teststatistik ist 
\begin{equation*}
T:= \frac{\overline{X}_n-\vartheta_0}{S/\sqrt{n}} \sim t_{n-1}, \quad \text{unter $P_{\vartheta_0}$}
\end{equation*}

\subsection{Gepaarte Stichproben}
Seien $X_1, \dots, X_n \sim \mathcal{N}(\mu_X, \sigma^2)$ und $Y_1, \dots, Y_n \sim \mathcal{N}(\mu_Y, \sigma^2)$ Daten von gepaarten Stichproben mit gleicher Varianz $\sigma$ . Wir definieren $\Delta_i := X_i - Y_i$
\subsubsection{Bekannte Varianz}
Ist die Varianz bekannt, so sind $\Delta_i \overset{i.i.d.}{\sim} \mathcal{N}(\mu_X - \mu_Y, 2 \sigma^2)$ und die Teststatistik ist
\begin{equation*}
\frac{(\overline{X}_n - \overline{Y}_n) - (\mu_X - \mu_Y)}{\sqrt{2}\sigma / \sqrt{n}} \sim \mathcal{N}(0,1)
\end{equation*}

\subsubsection{Unbekante Varianz}
Ist die Varianz unbekannt, so schätzt man zunächst die Varianz als
\begin{equation*}
S^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left( \Delta_i - \overline{\Delta}_i \right)^2
\end{equation*}
Dann ist 
\begin{equation*}
\frac{(\overline{X}_n - \overline{Y}_n) - (\mu_X - \mu_Y)}{ S / \sqrt{n}} \sim t_{n-1}
\end{equation*}

\subsection{Ungepaarte Stichproben}
\subsubsection{Bekannte Varianz}
Seien $X_1, \dots, X_n \overset{i.i.d.}{\sim} \mathcal{N}(\mu_X, \sigma^2)$ und $Y_1, \dots, Y_m \overset{i.i.d.}{\sim} \mathcal{N}(\mu_Y, \sigma^2)$ mit bekannter und identischer Varianz $\sigma^2$. Dann machen wir einen Z-Test mit der Teststatistik:
\begin{equation*}
\frac{(\overline{X}_n- \overline{Y}_m) - (\mu_X - \mu_Y)}{\sigma \sqrt{\frac{1}{n} + \frac{1}{m}}} \sim \mathcal{N}(0,1)
\end{equation*}
\subsubsection{Unbekannte Varianz}
Seien $X_1, \dots, X_n \overset{i.i.d.}{\sim} \mathcal{N}(\mu_X, \sigma^2)$ und $Y_1, \dots, Y_m \overset{i.i.d.}{\sim} \mathcal{N}(\mu_Y, \sigma^2)$ mit identischer, aber unbekannter Varianz $\sigma^2$. Wir berechnen zuerst die beiden empirischen Varianzen:
\begin{equation*}
S_X^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \overline{X}_n)^2 \qquad\text{und}\qquad S_Y^2 = \frac{1}{m-1}\sum_{i=1}^{m}(Y_i - \overline{Y}_n)^2
\end{equation*}
Mit 
\begin{equation*}
S^2 = \frac{1}{m+n-2}\left( \left( n - 1 \right) S_X^2 + \left( m - 1 \right) S_Y^2 \right)
\end{equation*}
Ist die Teststatistik
\begin{equation*}
\frac{(\overline{X}_n- \overline{Y}_m) - (\mu_X - \mu_Y)}{S \sqrt{\frac{1}{n} + \frac{1}{m}}} \sim t_{n+m-2}
\end{equation*}

\section{Satz 7.1}
Seien $X_1, \dots, X_m \overset{i.i.d.}{\sim} \mathcal{N}(\mu, \sigma^2)$. Dann gilt:
\begin{enumerate}
\item $\overline{X}_n \sim \mathcal{N}(\mu,\frac{1}{n}\sigma^2)$ und $\dfrac{\overline{X}_n - \mu}{\sigma / \sqrt{n}} \sim \mathcal{N}(0,1)$.
\item $\dfrac{n-1}{\sigma^2}S^2 = \dfrac{1}{\sigma^2}\displaystyle\sum_{i=1}^{n}(X_i - \overline{X}_n)^2$ ist $\chi^2$-verteilt mit $n-1$ Freiheitsgraden.
\item $\overline{X}_n$ und $S^2$ sind unabhängig.
\item Der Quotient
\begin{equation*}
\frac{\overline{X}_n - \mu}{S/\sqrt{n}} = \cfrac{\frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}}}{S/ \sigma} = \cfrac{\dfrac{\overline{X}_n - \mu}{\sigma / \sqrt{n}}}{\sqrt{\dfrac{1}{n-1}\dfrac{n-1}{\sigma^2}S^2}}
\end{equation*}
ist $t$-verteilt mit $n-1$ Freiheitsgraden.
\end{enumerate}

\end{document}
